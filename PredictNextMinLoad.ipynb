{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+---------+\n",
      "|          timewindow|      ipaddress|HitperMin|\n",
      "+--------------------+---------------+---------+\n",
      "|[2015-07-22 05:00...|117.200.191.192|        1|\n",
      "|[2015-07-22 05:00...| 106.77.203.224|        1|\n",
      "|[2015-07-22 05:00...| 122.15.164.218|        3|\n",
      "|[2015-07-22 05:00...|  107.150.4.153|        1|\n",
      "|[2015-07-22 05:00...|  103.242.156.9|        1|\n",
      "|[2015-07-22 05:00...|   106.67.99.24|        1|\n",
      "|[2015-07-22 05:00...|107.167.108.212|        2|\n",
      "|[2015-07-22 05:00...| 122.166.231.76|        1|\n",
      "|[2015-07-22 05:00...| 182.68.216.254|        2|\n",
      "|[2015-07-22 05:00...|117.234.213.177|        1|\n",
      "|[2015-07-22 05:00...|  49.205.99.169|        1|\n",
      "|[2015-07-22 05:00...| 117.211.43.234|        2|\n",
      "|[2015-07-22 05:00...|  49.207.236.65|        1|\n",
      "|[2015-07-22 05:00...|107.167.108.131|        2|\n",
      "|[2015-07-22 05:00...|  128.185.3.222|        1|\n",
      "|[2015-07-22 05:01...| 115.113.117.48|       14|\n",
      "|[2015-07-22 05:01...|     1.39.13.51|        3|\n",
      "|[2015-07-22 05:01...|  203.122.41.18|        5|\n",
      "|[2015-07-22 05:01...| 117.196.181.12|        6|\n",
      "|[2015-07-22 05:01...|  116.203.5.226|        5|\n",
      "+--------------------+---------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Machine learning for Load Prediction\") \\\n",
    "    .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "# Load the data by creating rdd\n",
    "rdd = sc.textFile('/home/hassan/Side_Projects/WeblogChallenge/data/2015_07_22_mktplace_shop_web_log_sample.log')\n",
    "# split the data into columns\n",
    "rdd = rdd.map(lambda line: line.split(\" \"))\n",
    "# ====================================\n",
    "# Manipulating data\n",
    "# ====================================\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "#Map the RDD to a DF for better performance\n",
    "mainDF = rdd.map(lambda line: Row(timestamp=line[0], ipaddress=line[2].split(':')[0],url=line[12])).toDF()\n",
    "# convert timestamps from string to timestamp datatype\n",
    "mainDF = mainDF.withColumn('timestamp', mainDF['timestamp'].cast(TimestampType()))\n",
    "\n",
    "#get count of hit within window of 60 every seccond\n",
    "loadperMinDF = mainDF.select(window(\"timestamp\", \"60 seconds\").alias('timewindow'),'timestamp',\"ipaddress\").groupBy('timewindow').count().withColumnRenamed('count', 'HitperMin')\n",
    "# get count of hit per IP\n",
    "countdDF = mainDF.select(window(\"timestamp\", \"60 seconds\").alias('timewindow'),'timestamp',\"ipaddress\").groupBy('timewindow','ipaddress').count().withColumnRenamed('count', 'HitperMin')\n",
    "countdDF.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+-------------------+--------------------+-------------------+---------+\n",
      "|timewindow                                   |stdOfHitPerMinPerIP|meanOfHitPerMinPerIP|maxOfHitPerMinPerIP|HitperMin|\n",
      "+---------------------------------------------+-------------------+--------------------+-------------------+---------+\n",
      "|[2015-07-21 22:40:00.0,2015-07-21 22:41:00.0]|37.63654992233117  |6.183619550858652   |978                |4681     |\n",
      "|[2015-07-21 22:41:00.0,2015-07-21 22:42:00.0]|45.16269677704943  |8.580278128950695   |846                |6787     |\n",
      "|[2015-07-21 22:42:00.0,2015-07-21 22:43:00.0]|41.00957913182911  |7.097839898348157   |915                |5586     |\n",
      "|[2015-07-21 22:43:00.0,2015-07-21 22:44:00.0]|21.876291841819434 |5.738181818181818   |366                |4734     |\n",
      "|[2015-07-21 22:44:00.0,2015-07-21 22:45:00.0]|19.665877050690277 |5.591397849462366   |374                |4680     |\n",
      "|[2015-07-21 22:45:00.0,2015-07-21 22:46:00.0]|2.0763215336529917 |1.8888888888888888  |18                 |323      |\n",
      "|[2015-07-22 01:09:00.0,2015-07-22 01:10:00.0]|NaN                |1.0                 |1                  |1        |\n",
      "|[2015-07-22 01:10:00.0,2015-07-22 01:11:00.0]|13.95908526268474  |5.097930338213024   |355                |10099    |\n",
      "|[2015-07-22 01:11:00.0,2015-07-22 01:12:00.0]|16.928535755315508 |6.049766718506999   |346                |11670    |\n",
      "|[2015-07-22 01:12:00.0,2015-07-22 01:13:00.0]|22.19877473979011  |6.37948984903696    |539                |12255    |\n",
      "|[2015-07-22 01:13:00.0,2015-07-22 01:14:00.0]|25.967580688670626 |6.771067415730337   |467                |14463    |\n",
      "|[2015-07-22 01:14:00.0,2015-07-22 01:15:00.0]|18.09106237421149  |5.919556840077071   |575                |12289    |\n",
      "|[2015-07-22 01:15:00.0,2015-07-22 01:16:00.0]|6.7884838863006856 |2.982490272373541   |75                 |1533     |\n",
      "|[2015-07-22 02:54:00.0,2015-07-22 02:55:00.0]|0.0                |1.0                 |1                  |4        |\n",
      "|[2015-07-22 02:55:00.0,2015-07-22 02:56:00.0]|24.397729320063686 |5.4246753246753245  |1058               |12531    |\n",
      "|[2015-07-22 02:56:00.0,2015-07-22 02:57:00.0]|13.954862803139653 |5.335841584158416   |493                |13473    |\n",
      "|[2015-07-22 02:57:00.0,2015-07-22 02:58:00.0]|11.228685780753388 |5.507252382925818   |152                |13289    |\n",
      "|[2015-07-22 02:58:00.0,2015-07-22 02:59:00.0]|10.784985916123745 |5.006719865602688   |274                |11921    |\n",
      "|[2015-07-22 02:59:00.0,2015-07-22 03:00:00.0]|11.13514600581905  |5.062182741116751   |278                |11967    |\n",
      "|[2015-07-22 03:00:00.0,2015-07-22 03:01:00.0]|4.046688058258602  |2.400749063670412   |59                 |1282     |\n",
      "+---------------------------------------------+-------------------+--------------------+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# computing mean ,std and max of hit counts per IP within 60 seccond window as \n",
    "# features of each 60 seccond window \n",
    "# these features can be used for perdicting the next load in next minute\n",
    "Feature1 = countdDF.groupBy(\"timewindow\").agg(stddev('HitperMin').alias(\"stdOfHitPerMinPerIP\"))\n",
    "Feature2 = countdDF.groupBy(\"timewindow\").agg(mean('HitperMin').alias(\"meanOfHitPerMinPerIP\"))\n",
    "Feature3 = countdDF.groupBy(\"timewindow\").agg(max('HitperMin').alias(\"maxOfHitPerMinPerIP\"))\n",
    "\n",
    "Features = Feature1.join(Feature2,[\"timewindow\"])\n",
    "Features = Features.join(Feature3,[\"timewindow\"])\n",
    "Features = Features.join(loadperMinDF,[\"timewindow\"])\n",
    "Features = Features.orderBy('timewindow', ascending=True)\n",
    "Features.show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+-------------------+------------------+\n",
      "|          timewindow|stdOfHitPerSecPerIP|meanOfHitPerSecPerIP|maxOfHitPerSecPerIP|         HitperSec|\n",
      "+--------------------+-------------------+--------------------+-------------------+------------------+\n",
      "|[2015-07-21 22:40...| 0.6272758320388528|  0.1030603258476442|               16.3| 78.01666666666667|\n",
      "|[2015-07-21 22:41...| 0.7527116129508239| 0.14300463548251158|               14.1|113.11666666666666|\n",
      "|[2015-07-21 22:42...| 0.6834929855304852| 0.11829733163913596|              15.25|              93.1|\n",
      "|[2015-07-21 22:43...| 0.3646048640303239| 0.09563636363636364|                6.1|              78.9|\n",
      "|[2015-07-21 22:44...| 0.3277646175115046|  0.0931899641577061|  6.233333333333333|              78.0|\n",
      "+--------------------+-------------------+--------------------+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Divide hit coutns by 60 to become hit per seccond\n",
    "Features = Features.withColumn(\"stdOfHitPerSecPerIP\", col(\"stdOfHitPerMinPerIP\")/60.0) \\\n",
    "   .withColumn(\"meanOfHitPerSecPerIP\", col(\"meanOfHitPerMinPerIP\")/60.0) \\\n",
    "   .withColumn(\"maxOfHitPerSecPerIP\", col(\"maxOfHitPerMinPerIP\")/60.0) \\\n",
    "   .withColumn(\"HitperSec\", col(\"HitperMin\")/60.0)\n",
    "Features = Features.select(\"timewindow\",\"stdOfHitPerSecPerIP\",\"meanOfHitPerSecPerIP\",\"maxOfHitPerSecPerIP\",\"HitperSec\")\n",
    "Features.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+--------------------+--------------------+-----+\n",
      "|          timewindow|stdOfHitPerSecPerIP|meanOfHitPerSecPerIP| maxOfHitPerSecPerIP|           HitperSec|tagId|\n",
      "+--------------------+-------------------+--------------------+--------------------+--------------------+-----+\n",
      "|[2015-07-21 22:40...| 0.6272758320388528|  0.1030603258476442|                16.3|   78.01666666666667|  0.0|\n",
      "|[2015-07-21 22:41...| 0.7527116129508239| 0.14300463548251158|                14.1|  113.11666666666666|  1.0|\n",
      "|[2015-07-21 22:42...| 0.6834929855304852| 0.11829733163913596|               15.25|                93.1|  2.0|\n",
      "|[2015-07-21 22:43...| 0.3646048640303239| 0.09563636363636364|                 6.1|                78.9|  3.0|\n",
      "|[2015-07-21 22:44...| 0.3277646175115046|  0.0931899641577061|   6.233333333333333|                78.0|  4.0|\n",
      "|[2015-07-21 22:45...|0.03460535889421653| 0.03148148148148148|                 0.3|   5.383333333333334|  5.0|\n",
      "|[2015-07-22 01:09...|                NaN|0.016666666666666666|0.016666666666666666|0.016666666666666666|  6.0|\n",
      "|[2015-07-22 01:10...|0.23265142104474565| 0.08496550563688372|   5.916666666666667|  168.31666666666666|  7.0|\n",
      "|[2015-07-22 01:11...| 0.2821422625885918| 0.10082944530844998|   5.766666666666667|               194.5|  8.0|\n",
      "|[2015-07-22 01:12...|0.36997957899650186| 0.10632483081728267|   8.983333333333333|              204.25|  9.0|\n",
      "+--------------------+-------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get id for each window\n",
    "Features = Features.withColumn(\"tagId\", monotonically_increasing_id().cast(\"double\"))\n",
    "Features.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|stdOfHitPerSecPerIP|meanOfHitPerSecPerIP| maxOfHitPerSecPerIP|           HitperSec|   LoadInNextOnedMin|\n",
      "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| 0.6272758320388528|  0.1030603258476442|                16.3|   78.01666666666667|  113.11666666666666|\n",
      "| 0.7527116129508239| 0.14300463548251158|                14.1|  113.11666666666666|                93.1|\n",
      "| 0.6834929855304852| 0.11829733163913596|               15.25|                93.1|                78.9|\n",
      "| 0.3646048640303239| 0.09563636363636364|                 6.1|                78.9|                78.0|\n",
      "| 0.3277646175115046|  0.0931899641577061|   6.233333333333333|                78.0|   5.383333333333334|\n",
      "|0.03460535889421653| 0.03148148148148148|                 0.3|   5.383333333333334|0.016666666666666666|\n",
      "|                NaN|0.016666666666666666|0.016666666666666666|0.016666666666666666|  168.31666666666666|\n",
      "|0.23265142104474565| 0.08496550563688372|   5.916666666666667|  168.31666666666666|               194.5|\n",
      "| 0.2821422625885918| 0.10082944530844998|   5.766666666666667|               194.5|              204.25|\n",
      "|0.36997957899650186| 0.10632483081728267|   8.983333333333333|              204.25|              241.05|\n",
      "+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get hit per sec of each next 60 sec window \n",
    "# we will use this as the label for training model\n",
    "from pyspark.sql.window import Window\n",
    "w = Window.orderBy('tagId').rowsBetween(1,1) \n",
    "avgHit = avg(Features['HitperSec']).over(w) \n",
    "NextloadDf = Features.select(Features['stdOfHitPerSecPerIP'],Features['meanOfHitPerSecPerIP'],Features['maxOfHitPerSecPerIP'],Features['HitperSec'],avgHit.alias(\"LoadInNextOnedMin\"))\n",
    "NextloadDf.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(label=113.11666666666666, features=DenseVector([0.6273, 0.1031, 16.3, 78.0167]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing null values\n",
    "NextloadDf = NextloadDf.na.drop(subset=[\"stdOfHitPerSecPerIP\"])\n",
    "NextloadDf = NextloadDf.na.drop(subset=[\"LoadInNextOnedMin\"])\n",
    "NextloadDf = NextloadDf.na.drop(subset=[\"HitperSec\"])\n",
    "\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "# Define the `input_data` \n",
    "input_data = NextloadDf.rdd.map(lambda x: (x[4], DenseVector(x[:4])))\n",
    "dataFrameInputdata = spark.createDataFrame(input_data, [\"label\", \"features\"])\n",
    "dataFrameInputdata.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training a linear regression Model\n",
    "train_data, test_data = dataFrameInputdata.randomSplit([.8,.2])\n",
    "from pyspark.ml.regression import LinearRegression,RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(numTrees=100, maxDepth=10)\n",
    "linearModel = rf.fit(train_data)\n",
    "\n",
    "#lr = LinearRegression(labelCol=\"label\", maxIter=100, regParam=0.3, elasticNetParam=0.8)\n",
    "# Fit the data to the model\n",
    "#linearModel = lr.fit(train_data)\n",
    "\n",
    "\n",
    "predicted = linearModel.transform(test_data)\n",
    "predictions = predicted.select(\"prediction\").rdd.map(lambda x: x[0])\n",
    "labels = predicted.select(\"label\").rdd.map(lambda x: x[0])\n",
    "predictionAndLabel = predictions.zip(labels).collect()\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean abs error is:  65.400781239\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "error =[]\n",
    "for a in predictionAndLabel:\n",
    "   error.append(np.abs(a[0]-a[1]))\n",
    "    \n",
    "print 'mean abs error is: ',np.mean(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[69.80983333333323]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicting the load for the next minute  \n",
    "# becuase the last data record is the last 60 seccond of data\n",
    "# we predict for the last record for predicting the next minute load\n",
    "\n",
    "with_id = NextloadDf.withColumn(\"_id\", monotonically_increasing_id())\n",
    "i = with_id.select(max(\"_id\")).first()[0]\n",
    "last_item = with_id.where(col(\"_id\") == i).drop(\"_id\")\n",
    "input_data = last_item.rdd.map(lambda x: (x[4], DenseVector(x[:4])))\n",
    "dataFrameInputdata = spark.createDataFrame(input_data, [\"label\", \"features\"])\n",
    "predicted = linearModel.transform(dataFrameInputdata)\n",
    "predictions = predicted.select(\"prediction\").rdd.map(lambda x: x[0])\n",
    "labels = predicted.select(\"label\").rdd.map(lambda x: x[0])\n",
    "predictions = predictions.collect()\n",
    "predictions[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
